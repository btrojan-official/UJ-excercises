{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhfZmLsgJMZZ"
      },
      "source": [
        "# PyTorch - ciąg dalszy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dF7fkpn0xNT"
      },
      "source": [
        "## Przykład: trenowanie prostego modelu\n",
        "Przykład jest zaczerpnięty z [oficjalnego tutoriala](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "v01LISTHE728"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda, Compose\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fl-FeF35OJ2L",
        "outputId": "8245c184-d353-42af-8bdb-31ce5ca10e22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:09<00:00, 2860768.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 84497.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:02<00:00, 1850960.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 6497826.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Shape of X [N, C, H, W]:  torch.Size([64, 1, 28, 28])\n",
            "Shape of y:  torch.Size([64]) torch.int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Download training data from open datasets.\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# Download test data from open datasets.\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "for X, y in test_dataloader:\n",
        "    print(\"Shape of X [N, C, H, W]: \", X.shape)\n",
        "    print(\"Shape of y: \", y.shape, y.dtype)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfgPIdqeUf20"
      },
      "source": [
        "Dygresja: DataLoadery.\n",
        "\n",
        "Gdy używamy CPU i GPU, to nie jest trywialne jak zaimplementować ładowanie danych i podawanie ich do modelu, by zarówno wykorzystanie CPU jak i GPU było optymalne. Przy naiwnej implementacji możemy mieć następujący efekt:\n",
        "![](https://i.imgur.com/b3Oxcf1.png)\n",
        "\n",
        "Wolelibyśmy, żeby ten obrazek wyglądał tak:\n",
        "![](https://i.imgur.com/QRGSW6Z.png)\n",
        "\n",
        "Na szczęście PyTorch robi całą tę brudną robotę za nas, i udostępnia klasy `Dataset` i `DataLoader`, które umożliwiają efektywne wielowątkowe ładowanie danych, batchowanie itp."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## I moved my dropout here, so I can test if it works in real, sequential torch model\n",
        "\n",
        "class DropoutFunction(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, p):\n",
        "        p = p.to(input.device)\n",
        "        D = (torch.rand(input.shape).to(input.device) > p).float()\n",
        "        ctx.save_for_backward(input, D, p)\n",
        "        return (input * D) / (1 - p)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        input, D, p = ctx.saved_tensors\n",
        "        A = grad_output * D\n",
        "        return A / (1-p), torch.tensor([0])\n",
        "\n",
        "class MyDropout(nn.Module):\n",
        "    def __init__(self, p=0.25):\n",
        "        super(MyDropout, self).__init__()\n",
        "        self.p = torch.tensor([p])\n",
        "\n",
        "    def forward(self,x):\n",
        "        if self.training:\n",
        "            return DropoutFunction.apply(x, self.p)\n",
        "        else:\n",
        "            return x"
      ],
      "metadata": {
        "id": "Hukx1O4fRKAX"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8m5NSibmO8kB",
        "outputId": "c91955e2-3dfb-44eb-a8ed-911713041d67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU()\n",
            "    (3): MyDropout()\n",
            "    (4): Linear(in_features=512, out_features=356, bias=True)\n",
            "    (5): BatchNorm1d(356, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (6): ReLU()\n",
            "    (7): MyDropout()\n",
            "    (8): Linear(in_features=356, out_features=128, bias=True)\n",
            "    (9): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (10): ReLU()\n",
            "    (11): MyDropout()\n",
            "    (12): Linear(in_features=128, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Get cpu or gpu device for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using {} device\".format(device))\n",
        "\n",
        "# Define model\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.BatchNorm1d(512),  # Batch Normalization after the first Linear layer\n",
        "            nn.ReLU(),\n",
        "            MyDropout(0.2),\n",
        "            nn.Linear(512, 356),\n",
        "            nn.BatchNorm1d(356),  # Batch Normalization after the second Linear layer\n",
        "            nn.ReLU(),\n",
        "            MyDropout(0.3),\n",
        "            nn.Linear(356, 128),\n",
        "            nn.BatchNorm1d(128),  # Batch Normalization after the third Linear layer\n",
        "            nn.ReLU(),\n",
        "            MyDropout(0.2),\n",
        "            nn.Linear(128, 10),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "9k8MKhVJUjY1"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "DHptlXciUkP8"
      },
      "outputs": [],
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()  # ważny krok! nie chcemy żeby gradienty z różnych kroków się na siebie \"nakładały\"\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 400 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test(dataloader, model):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= size\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEC07R-AUtvM",
        "outputId": "de1b9200-8731-47bd-d9d3-244e35a0404a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.402184  [    0/60000]\n",
            "loss: 0.678566  [25600/60000]\n",
            "loss: 0.553627  [51200/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.8%, Avg loss: 0.006879 \n",
            "\n",
            "Epoch finished in 10.9s.\n",
            "\n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.380755  [    0/60000]\n",
            "loss: 0.440382  [25600/60000]\n",
            "loss: 0.445928  [51200/60000]\n",
            "Test Error: \n",
            " Accuracy: 85.7%, Avg loss: 0.006277 \n",
            "\n",
            "Epoch finished in 11.1s.\n",
            "\n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.351271  [    0/60000]\n",
            "loss: 0.348417  [25600/60000]\n",
            "loss: 0.416932  [51200/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.0%, Avg loss: 0.005561 \n",
            "\n",
            "Epoch finished in 12.9s.\n",
            "\n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.238435  [    0/60000]\n",
            "loss: 0.310225  [25600/60000]\n",
            "loss: 0.333073  [51200/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.7%, Avg loss: 0.005299 \n",
            "\n",
            "Epoch finished in 15.7s.\n",
            "\n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.231587  [    0/60000]\n",
            "loss: 0.419916  [25600/60000]\n",
            "loss: 0.366704  [51200/60000]\n",
            "Test Error: \n",
            " Accuracy: 87.7%, Avg loss: 0.005245 \n",
            "\n",
            "Epoch finished in 12.7s.\n",
            "\n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.249259  [    0/60000]\n"
          ]
        }
      ],
      "source": [
        "epochs = 30\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    start_time = time.time()\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    test(test_dataloader, model)\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f\"Epoch finished in {elapsed_time:.1f}s.\\n\\n\")\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23onWc4n1dTB"
      },
      "source": [
        "## Przykład: różniczkowanie mnożenia\n",
        "Przypomnijmy sobie regułę łańcuchową liczenia pochodnych. Jeśli mamy:\n",
        "$$ L(x) = g(f(x)), $$\n",
        "to wtedy:\n",
        "\n",
        "$$ \\frac{\\partial L(x)}{dx} = \\frac{\\partial L(x)}{\\partial f(x)}\\frac{\\partial f(x)}{\\partial x} $$\n",
        "\n",
        "W kontekście automatycznego różniczkowania w PyTorchu kluczowa jest tu właściwość, że do policzenia gradientu nie musimy nic wiedzieć o funkcji $g$ o ile tylko znamy $\\frac{\\partial L(x)}{\\partial f(x)}$. **Każdy moduł wie, jak policzyć swój gradient i dzięki temu można łańcuchowo liczyć pochodne skomplikowanych funkcji.**\n",
        "\n",
        "\n",
        "W PyTorchu każda funkcja, której używamy, ma zaimplementowane dwa podmoduły:\n",
        "* **Forward** - na podstawie podanego $x$ potrafi obliczyć $f(x)$.\n",
        "* **Backward** - na podstawie podanego $\\frac{\\partial L(x)}{\\partial f(x)}$ potrafi policzyć $\\frac{\\partial L(x)}{\\partial x}$.\n",
        "\n",
        "Obiekt, który reprezentuje w pytorchu funkcję, która potrafi policzyć swoją pochodną, dziedziczy po klasie `torch.autograd.Function`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NF5WFpWw4MWx"
      },
      "source": [
        "Chcemy zaimplementować od nowa w PyTorchu fukcję $f(a, b) = a \\cdot b$, która potrafi policzyć swoje pochodne.\n",
        "\n",
        "W efekcie implementujemy obiekt typu `torch.autograd.Function` z metodami:\n",
        "* **Forward**\n",
        "    1. Dostaje na wejściu `a` oraz `b`\n",
        "    1. Zapamiętuje `a` oraz `b`, które przydadzą się później przy liczeniu pochodnej\n",
        "    2. Zwraca `a * b`\n",
        "* **Backward**\n",
        "    1. Dostaje na wejściu `grad_output` reprezentujące wartość $\\frac{\\partial L(x)}{\\partial f(a, b)}$.\n",
        "    2. Wyjmuje z pamięci `a` oraz `b`.\n",
        "    3. Liczy swoją pochodną po a: $\\frac{\\partial f(a, b)}{\\partial a} = \\frac{ \\partial ab }{\\partial a} = b$\n",
        "    4. Liczy swoją pochodną po b: $\\frac{\\partial f(a, b)}{\\partial b} = \\frac{ \\partial ab }{\\partial b} = a$\n",
        "    5. Zwraca pochodne $\\frac{\\partial L(x)}{\\partial f(a, b)} \\frac{\\partial f(a, b)}{\\partial a}$ oraz $\\frac{\\partial L(x)}{\\partial f(a, b)} \\frac{\\partial f(a, b)}{db}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9cbtu2E5Dal"
      },
      "outputs": [],
      "source": [
        "class MyProduct(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(self, a, b):\n",
        "        self.save_for_backward(a, b)\n",
        "        return a * b\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(self, grad_output):\n",
        "        # Wyjmujemy z pamięci a oraz b\n",
        "        a, b = self.saved_tensors\n",
        "        # Liczymy pochodną po a\n",
        "        a_grad = b\n",
        "        # Liczymy pochodną po b\n",
        "        b_grad = a\n",
        "\n",
        "        # Zwracamy \"łańcuchowe\" pochodne\n",
        "        return grad_output * a_grad, grad_output * b_grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEIrjIps5tmy"
      },
      "outputs": [],
      "source": [
        "prod = MyProduct.apply  # bierzemy faktyczną funkcję, którą możemy odpalać na tensorach\n",
        "\n",
        "x = torch.tensor(2., requires_grad=True)\n",
        "y = torch.tensor(3., requires_grad=True)\n",
        "z = torch.tensor(5., requires_grad=True)\n",
        "\n",
        "result = prod(prod(x, y), z)  # sekwencyjne użycie - pytorch sam ogarnie składanie operatorów\n",
        "\n",
        "result.backward()  # uruchamiamy backpropagation\n",
        "print(x.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8MW4vbx9nS8"
      },
      "source": [
        "## Zadanie"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5J_LShFI9uZD"
      },
      "source": [
        "**Zadanie 1.** (*20% punktów*) Proszę zaimplementować funkcje `MySum`, `MySigmoid`, `MyRelu`, dziedziczące po `torch.autograd.Function` i implementujące odpowiednio operacje: sumy dwóch tensorów, operacji $\\sigma$ (sigmoidy), aktywacji relu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45KMedFf-BtK"
      },
      "outputs": [],
      "source": [
        "class MySum(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(self,a,b):\n",
        "        self.save_for_backward(a,b)\n",
        "        return a + b\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(self, grad_output):\n",
        "        a,b = self.saved_tensors\n",
        "\n",
        "        grad_a = 1\n",
        "        grad_b = 1\n",
        "\n",
        "        return grad_output * grad_a, grad_output * grad_b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXqmhWrX-jm3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class MySigmoid(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, a):\n",
        "        sigmoid_value = 1 / (1 + torch.exp(-a))\n",
        "\n",
        "        ctx.save_for_backward(a)\n",
        "\n",
        "        return sigmoid_value\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        a, = ctx.saved_tensors\n",
        "\n",
        "        sigmoid_value = 1 / (1 + torch.exp(-a))\n",
        "\n",
        "        grad_a = sigmoid_value * (1 - sigmoid_value)\n",
        "\n",
        "        return grad_output * grad_a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2casZTfYA-p_"
      },
      "outputs": [],
      "source": [
        "class MyRelu(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(self,a):\n",
        "        self.save_for_backward(a)\n",
        "        return torch.maximum(a,torch.tensor(0.0))\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(self, grad_output):\n",
        "        a = self.saved_tensors\n",
        "\n",
        "        grad_a = (x > 0).float()\n",
        "\n",
        "        return grad_output * grad_a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQnbfmYXCHYc"
      },
      "outputs": [],
      "source": [
        "prod = MySigmoid.apply  # bierzemy faktyczną funkcję, którą możemy odpalać na tensorach\n",
        "\n",
        "x = torch.tensor(10., requires_grad=True)\n",
        "\n",
        "result = prod(x)\n",
        "\n",
        "result.backward()\n",
        "print(x.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szuTZr8s_rPa"
      },
      "source": [
        "## Przykład: moduły\n",
        "\n",
        "Moduły (`nn.Module`) to coś więcej niż funkcje (`torch.autograd.Function`). Moduły zarządzają też parametrami i są podstawowymi cegiełkami, z których budujemy sieć neuronową. Np. możemy mieć moduł \"warstwa liniowa\", \"warstwa konwolucyjna\" itp.\n",
        "\n",
        "Typowo, moduł w środku woła funkcje (`torch.autograd.Function`), by potem mogła zostać wykonana propagacja wsteczna.\n",
        "\n",
        "Prześledźmy poniżej przykład z [dokumentacji PyTorcha](https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd), gdzie jest zaimplementowana warstwa liniowa. *Uwaga*: jest to tylko przykład dla celów edukacyjnych, a nie faktyczna implementacja użyta w bibliotece PyTorch.\n",
        "\n",
        "Ponieważ logika liczenia pochodnych jest już zaszyta w funkcjach (`torch.autograd.Function`), to w module wystarczy zaimplementować tylko funkcję `forward`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jN9Qk2Y__uz0"
      },
      "outputs": [],
      "source": [
        "# Inherit from Function\n",
        "class LinearFunction(torch.autograd.Function):\n",
        "\n",
        "    # Note that both forward and backward are @staticmethods\n",
        "    @staticmethod\n",
        "    # bias is an optional argument\n",
        "    def forward(ctx, input, weight, bias=None):\n",
        "        ctx.save_for_backward(input, weight, bias)\n",
        "        output = input.mm(weight.t())\n",
        "        if bias is not None:\n",
        "            output += bias.unsqueeze(0).expand_as(output)\n",
        "        return output\n",
        "\n",
        "    # This function has only a single output, so it gets only one gradient\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        # This is a pattern that is very convenient - at the top of backward\n",
        "        # unpack saved_tensors and initialize all gradients w.r.t. inputs to\n",
        "        # None. Thanks to the fact that additional trailing Nones are\n",
        "        # ignored, the return statement is simple even when the function has\n",
        "        # optional inputs.\n",
        "        input, weight, bias = ctx.saved_tensors\n",
        "        grad_input = grad_weight = grad_bias = None\n",
        "\n",
        "        # These needs_input_grad checks are optional and there only to\n",
        "        # improve efficiency. If you want to make your code simpler, you can\n",
        "        # skip them. Returning gradients for inputs that don't require it is\n",
        "        # not an error.\n",
        "        if ctx.needs_input_grad[0]:\n",
        "            grad_input = grad_output.mm(weight)\n",
        "        if ctx.needs_input_grad[1]:\n",
        "            grad_weight = grad_output.t().mm(input)\n",
        "        if bias is not None and ctx.needs_input_grad[2]:\n",
        "            grad_bias = grad_output.sum(0)\n",
        "\n",
        "        return grad_input, grad_weight, grad_bias\n",
        "\n",
        "linear = LinearFunction.apply"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kwBNLC5ZCbdA"
      },
      "outputs": [],
      "source": [
        "class Linear(nn.Module):\n",
        "    def __init__(self, input_features, output_features, bias=True):\n",
        "        super(Linear, self).__init__()\n",
        "        self.input_features = input_features\n",
        "        self.output_features = output_features\n",
        "\n",
        "        # nn.Parameter is a special kind of Tensor, that will get\n",
        "        # automatically registered as Module's parameter once it's assigned\n",
        "        # as an attribute. Parameters and buffers need to be registered, or\n",
        "        # they won't appear in .parameters() (doesn't apply to buffers), and\n",
        "        # won't be converted when e.g. .cuda() is called. You can use\n",
        "        # .register_buffer() to register buffers.\n",
        "        # nn.Parameters require gradients by default.\n",
        "        self.weight = nn.Parameter(torch.Tensor(output_features, input_features))\n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(torch.Tensor(output_features))\n",
        "        else:\n",
        "            # You should always register all possible parameters, but the\n",
        "            # optional ones can be None if you want.\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "        # Not a very smart way to initialize weights\n",
        "        self.weight.data.uniform_(-0.1, 0.1)\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.uniform_(-0.1, 0.1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # See the autograd section for explanation of what happens here.\n",
        "        return LinearFunction.apply(input, self.weight, self.bias)\n",
        "\n",
        "    def extra_repr(self):\n",
        "        # (Optional)Set the extra information about this module. You can test\n",
        "        # it by printing an object of this class.\n",
        "        return 'input_features={}, output_features={}, bias={}'.format(\n",
        "            self.input_features, self.output_features, self.bias is not None\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osIXGZkZCtdf",
        "outputId": "98f9a31c-7563-4f6e-f03e-6aaf3a69c07e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear(input_features=10, output_features=20, bias=True)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1029,  0.8311,  0.0362, -1.2362, -0.9078, -0.0261,  0.0188,  1.2537,\n",
              "          1.2374,  0.9751,  0.3222,  0.4445,  0.0967, -2.2882,  0.1282, -1.2846,\n",
              "          0.1086,  1.4780,  0.5609,  0.3851]],\n",
              "       grad_fn=<LinearFunctionBackward>)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "linear_layer = Linear(10, 20)\n",
        "print(linear_layer)\n",
        "\n",
        "linear_layer(torch.tensor([[1.,2.,3.,4.,5.,6.,7.,8.,9., 9.]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOVyqzC_JyAV"
      },
      "source": [
        "## Zadania"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMKB04_JJ7-L"
      },
      "source": [
        "**Zadanie 2.** (*40% punktów*) Proszę zaimplementować pytorchowe funkcję oraz moduł do liczenia dropoutu. Wartość $p$ dropoutu powinna być ustawialna jako parametr modułu. Moduł powinien w trybie ewaluacji nic nie robić, a w trybie treningu stosować dropout i skalować aktywacje razy $\\frac{1}{1 - p}$.\n",
        "\n",
        "W celu przetestowania modułu, proszę go dorzucić (bezpośrednio po pierwszej i drugiej warstwie liniowej) do przykładowej architektury przedstawionej wyżej (sekcja \"trenowanie prostego modelu\"):\n",
        "* gdy ustawimy `p=0`, nic nie powinno się zmienić;\n",
        "* gdy ustawimy `p=0.2`, ostateczny wynik na zbiorze testowym powinien być wyższy;\n",
        "* gdy ustawimy `p=1`, trenowanie powinno się popsuć.\n",
        "\n",
        "*Uwaga*. Nie wolno korzystać z gotowych pytorchowych funkcji typu `torch.nn.functional.dropout`.\n",
        "\n",
        "*Podpowiedź*. By zróżnicować zachowanie modułu w trakcie treningu i ewaluacji, [możemy wykorzystać](https://discuss.pytorch.org/t/how-do-i-customize-my-modules-behavior-for-train-and-eval/52693) atrybut `self.training`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OiBk9FcGJ6Gs",
        "outputId": "d1c6da0f-7741-4e0d-e3c9-b908d28f3366"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[3.3333e+00, 5.0000e+00, 6.6667e+00, 8.3333e+00, 5.0000e+00, 0.0000e+00,\n",
            "         3.8333e+01, 5.0000e+00, 0.0000e+00, 5.0000e+00, 3.8333e+01, 3.3333e+00,\n",
            "         0.0000e+00, 0.0000e+00, 5.7250e+03, 0.0000e+00, 0.0000e+00]])\n"
          ]
        }
      ],
      "source": [
        "class DropoutFunction(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, p):\n",
        "        D = (torch.rand(input.shape) > p).float()\n",
        "        ctx.save_for_backward(input, D, p)\n",
        "        return (input * D) / (1 - p)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        input, D, p = ctx.saved_tensors\n",
        "        A = grad_output * D\n",
        "        return A / (1-p)\n",
        "\n",
        "class MyDropout(nn.Module):\n",
        "    def __init__(self, p=0.25):\n",
        "        super(MyDropout, self).__init__()\n",
        "        self.p = torch.tensor([p])\n",
        "\n",
        "    def forward(self,x):\n",
        "        return DropoutFunction.apply(x, self.p)\n",
        "\n",
        "input = torch.tensor([[2,3,4,5,3,2,23,3,34,3,23,2,3233,3,3435,3,234]])\n",
        "mD = MyDropout(p=0.4)\n",
        "output = mD(input)\n",
        "\n",
        "print(output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiOCQj8wQgax"
      },
      "source": [
        "**Zadanie 3.** (*40% punktów*) Proszę zmodyfikować przykład z sekcji \"trenowanie prostego modelu\" tak, by osiągnąć accuracy na zbiorze testowym równe przynajmniej **88.5%**. Można używać gotowych modułów zaimplementowanych w pytorchu. Proszę pozostać przy warstwach liniowych, zabronione jest używanie warstw konwolucyjnych.\n",
        "\n",
        "*Podpowiedź*. Warto pobawić się następującymi elementami: dropout, batch norm, optymalizator (jakiś inny zamiast SGD).\n",
        "\n",
        "*Podpowiedź 2*. Proszę się przełączyć na GPU (Środowisko wykonawcze->Zmień typ środowiska wykonawczego->Akcelerator sprzętowy->GPU). Na tym modelu daje to około 2.5x przyspieszenie. (Przyspieszenie jest większe dla większych modeli!)\n",
        "\n",
        "*Podpowiedź 3*. W oryginalnej architekturze jest pewien niepotrzebny element, którego usunięcie od razu powoduje poprawienie modelu."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}